{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "VOCAB = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "firstLetter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - firstLetter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + firstLetter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "UNROLLINGS =10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, BATCH, UNROLLINGS):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = BATCH\n",
    "        self._num_unrollings = UNROLLINGS\n",
    "        segment = self._text_size // BATCH\n",
    "        self._cursor = [ offset * segment for offset in range(BATCH)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, VOCAB), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, BATCH, UNROLLINGS)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, VOCAB], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, VOCAB])\n",
    "    return b / np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NODES = 64\n",
    "BATCH = 64\n",
    "UNROLLINGS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = [tf.placeholder(tf.float32, shape=[BATCH, VOCAB]) for _ in range(UNROLLINGS + 1)]\n",
    "trainX = data[:UNROLLINGS]\n",
    "trainY = data[1:]  # labels are inputs shifted by one time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../Collection/img/lstm.png\" style=\"width: 600px;float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Forget Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../Collection/img/forget.png\" style=\"width: 600px;float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wfH = tf.Variable(tf.truncated_normal([NODES, NODES], -0.1, 0.1))\n",
    "wfX = tf.Variable(tf.truncated_normal([VOCAB, NODES], -0.1, 0.1))\n",
    "bf = tf.Variable(tf.zeros([1, NODES]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input Gate and Memory Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../Collection/img/input.png\" style=\"width: 600px;float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wiH = tf.Variable(tf.truncated_normal([NODES, NODES], -0.1, 0.1))\n",
    "wiX = tf.Variable(tf.truncated_normal([VOCAB, NODES], -0.1, 0.1))\n",
    "bi = tf.Variable(tf.zeros([1, NODES]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wcH = tf.Variable(tf.truncated_normal([NODES, NODES], -0.1, 0.1))\n",
    "wcX = tf.Variable(tf.truncated_normal([VOCAB, NODES], -0.1, 0.1))\n",
    "bc = tf.Variable(tf.zeros([1, NODES]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../Collection/img/output.png\" style=\"width: 600px;float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "woH = tf.Variable(tf.truncated_normal([NODES, NODES], -0.1, 0.1))\n",
    "woX = tf.Variable(tf.truncated_normal([VOCAB, NODES], -0.1, 0.1))\n",
    "bo = tf.Variable(tf.zeros([1, NODES]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../Collection/img/update.png\" style=\"width: 600px;float:left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LSTM(inputX, outputH, state):\n",
    "    fGate = tf.sigmoid(tf.matmul(outputH, wfH) + tf.matmul(inputX, wfX) + bf)\n",
    "    iGate = tf.sigmoid(tf.matmul(outputH, wiH) + tf.matmul(inputX, wiX) + bi)\n",
    "    oGate = tf.sigmoid(tf.matmul(outputH, woH) + tf.matmul(inputX, woX) + bo)\n",
    "    update = tf.tanh(tf.matmul(outputH, wcH) + tf.matmul(inputX, wcX) + bc)\n",
    "    state = fGate * state + iGate * tf.tanh(update)\n",
    "    output = oGate * tf.tanh(state)\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savedOutput = tf.Variable(tf.zeros([BATCH, NODES]), trainable=False)\n",
    "savedState = tf.Variable(tf.zeros([BATCH, NODES]), trainable=False)\n",
    "\n",
    "# Unrolled LSTM loop.\n",
    "outputs = list()\n",
    "outputH = savedOutput\n",
    "state = savedState\n",
    "for inputX in trainX:\n",
    "    output, state = LSTM(inputX, outputH, state)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([NODES, VOCAB], -0.1, 0.1))\n",
    "b = tf.Variable(tf.zeros([VOCAB]))\n",
    "\n",
    "# State saving across unrollings.\n",
    "with tf.control_dependencies([savedOutput.assign(outputH), savedState.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(trainY, 0), logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Optimizer.\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# Predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Sampling and validation eval: batch 1, no unrolling.\n",
    "sample_input = tf.placeholder(tf.float32, shape=[1, VOCAB])\n",
    "saved_sample_output = tf.Variable(tf.zeros([1, NODES]))\n",
    "saved_sample_state = tf.Variable(tf.zeros([1, NODES]))\n",
    "reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, NODES])), saved_sample_state.assign(tf.zeros([1, NODES])))\n",
    "sample_output, sample_state = LSTM(sample_input, saved_sample_output, saved_sample_state)\n",
    "with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292724 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "m meym ipyuage dznds ozwe bs ockfcxpznb wuznmitrrlfahraje  pyjbjrdfj  yfggint ff\n",
      "ebaqbie woqtaygw  druf assovzoxdehz dkcviol  gdxc fhrqrrtdx tmieyxuiii fidyhrevm\n",
      "gijuohhdftdi eltwdbkcqaqn  smmy  aenf xnascuabxvrneyd xitpy utcbnz hlaayvsiirbse\n",
      "zgdaiwqtrfilos o aavy  tiekdudeuthr isxxjesxsfe ni  rywhud fzjictyn jh   y trlth\n",
      "wawwcdabneoj cidgsnfo ug lce cl ick kpdvnpbatqpdd tase jfp rlbfmorqs  jvunm ve i\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 100: 2.661539 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.69\n",
      "Validation set perplexity: 13.28\n",
      "Average loss at step 200: 2.408464 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.62\n",
      "Validation set perplexity: 11.21\n",
      "Average loss at step 300: 2.353877 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.69\n",
      "Validation set perplexity: 10.98\n",
      "Average loss at step 400: 2.309455 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.69\n",
      "Validation set perplexity: 11.29\n",
      "Average loss at step 500: 2.273891 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Validation set perplexity: 10.52\n",
      "Average loss at step 600: 2.246184 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 10.48\n",
      "Average loss at step 700: 2.198360 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.92\n",
      "Validation set perplexity: 10.29\n",
      "Average loss at step 800: 2.174805 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.78\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 900: 2.168573 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.31\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 1000: 2.165349 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "================================================================================\n",
      "por fe cron to ine as exen omatyres gats fins sterdetil k com ted ac the th wato\n",
      "h fera dyatimmbe to tel ba hom calidig edhsel con impo far feecrzene tmal fiven \n",
      "ke no in ken ucteas aven id andut of hess bents lecof ei only the ies om pas een\n",
      "mult onsegotan ony scepure cas oredo posovin is latue reveev coh p noome in dagh\n",
      "he tan iner in for con ereta forttix bus of the rof m a inde teative he zenar pe\n",
      "================================================================================\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 1100: 2.125544 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 1200: 2.097642 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 1300: 2.084823 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.52\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 1400: 2.087137 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.20\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 1500: 2.097020 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 1600: 2.086116 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 1700: 2.063536 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 1800: 2.028884 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 1900: 2.007814 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 2000: 2.054605 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.20\n",
      "================================================================================\n",
      "he dingrecy hreigewest binad piticinlism wer two the unidered cinturing ayit rop\n",
      "jaen a is the wite and s uratie s val stivlend in lovisc his mppformy hos sevita\n",
      "ily ames ristys con hi whisqutialse nine ntares witts iis zerustraral smok one s\n",
      "ysuits bunincal aint dmonss andom four hus mspitiorn a inar with grepesicanguiti\n",
      "xbions trusight in tomed asto stonso coment and digh of garst nine possictsomdia\n",
      "================================================================================\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 2100: 2.056025 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 2200: 2.049246 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.88\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 2300: 2.006052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 2400: 2.020593 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 2500: 2.025907 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 2600: 2.008046 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.08\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 2700: 2.015685 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 2800: 2.007737 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 2900: 1.998285 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 3000: 1.991428 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "================================================================================\n",
      "y to semben risk ning debladutial and weghis periams nownne ald sects s panation\n",
      "onay k ot otedern sall twasion fto hat coulte umerisectixters histtire comple br\n",
      "ovelesular alaciof mapppeopplet reetland pracain dnos locess matide ea gacty bli\n",
      "vily guaty ushing it one prolabitansicafilly mested seloclestionsl aparke den kr\n",
      "rks as ovia land wo arlitwinve sptored from twishophering tooh anding baints tha\n",
      "================================================================================\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 3100: 1.974743 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 3200: 1.989520 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.56\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 3300: 1.985870 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 3400: 2.017220 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 3500: 1.997347 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 3600: 2.002065 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 3700: 1.986011 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 3800: 1.971307 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 3900: 1.976469 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 4000: 1.981902 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "================================================================================\n",
      "jega zero blany one zero mea zero somet feight the ormatic alab itorie feate ent\n",
      "it antry cyers compay ble whe ave t wee nin smivenited incomalks rive suchir s f\n",
      "lur centro the colladookching the comes fom sptero ine ir aby all s berer jist r\n",
      "e obach puit wha one famas and permaincurtary harmpere ish cand dikms threes ous\n",
      "ing twac ageorive was alose t por resomettra b catally boue felformi ass two mor\n",
      "================================================================================\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 4100: 1.970284 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 4200: 1.972955 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4300: 1.959259 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4400: 1.955820 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 4500: 1.946587 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4600: 1.948399 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 4700: 1.972058 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 4800: 1.957159 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4900: 1.964158 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5000: 1.922949 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.56\n",
      "================================================================================\n",
      "p russsaly mubimes is oentron of esto cupess no isim this halw bestambrain sigio\n",
      "col ave sout sfoure wouthnt berly for geres whot alttropuse dack ast pronainstal\n",
      "mintical diffe in on one nithat fublund a the ocack archic hims inine beplay bur\n",
      "be furbor a eveme treats pliofime forgnoson one sucasally foul wasuh gan is in o\n",
      "clan ripet two zero astaion to risasidd grentre five manters sor pupidork yite g\n",
      "================================================================================\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 5100: 1.937832 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 5200: 1.938705 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 5300: 1.914711 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 5400: 1.915283 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 5500: 1.922674 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 5600: 1.931756 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 5700: 1.924091 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 5800: 1.920504 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5900: 1.920681 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 6000: 1.898047 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.18\n",
      "================================================================================\n",
      "am enceroprally shit save in at many thork beage exicall and to a bodyseder atew\n",
      "an the lix cess afuter threy famunt jemper salsssix avialy remame kning the chan\n",
      "gions ise a reralrabwas asused motle becy as wilth the the high procre sl vouly \n",
      "verst ysitonalto ginsts wos prolion eaut comel ded seasshnajm oft and pisirude q\n",
      "epte the ea promestic ad a kploplactions comainald oequican fourisimst sysold sa\n",
      "================================================================================\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 6100: 1.911159 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 6200: 1.889473 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 6300: 1.900525 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 6400: 1.884950 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 6500: 1.910998 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 6600: 1.933414 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 6700: 1.923182 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6800: 1.943715 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6900: 1.924897 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 7000: 1.928831 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.48\n",
      "================================================================================\n",
      "aintish acts ome inthracters ast to dervalink mused aks is arly s six ad siccitt\n",
      "don ninestred mowlly requions ite daitian amubjacany oly ben the unine nineld pr\n",
      "tets a seven even one one six vairsion to free eigins the oanm i that grese of a\n",
      "ry pople jing ae mone mary an reefive sixes asos comst in wist thats anding toct\n",
      "y the suchiss cotoom pucteranay avess mandm ffor prity proplesian bord asetion r\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n"
     ]
    }
   ],
   "source": [
    "STEPS = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "print('Initialized')\n",
    "mean_loss = 0\n",
    "for step in range(STEPS):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(UNROLLINGS + 1):\n",
    "        feed_dict[data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "                feed = sample(random_distribution())\n",
    "                sentence = characters(feed)[0]\n",
    "                session.run(reset_sample_state)\n",
    "                for _ in range(79):\n",
    "                    prediction = session.run(sample_prediction, {sample_input: feed})\n",
    "                    feed = sample(prediction)\n",
    "                    sentence += characters(feed)[0]\n",
    "                print(sentence)\n",
    "            print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        session.run(reset_sample_state)\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "            b = valid_batches.next()\n",
    "            predictions = session.run(sample_prediction, {sample_input: b[0]})\n",
    "            valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
